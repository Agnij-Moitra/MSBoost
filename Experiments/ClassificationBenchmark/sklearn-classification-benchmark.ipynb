{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07ceaa8-9e14-4c04-bfcc-f0f18bf680c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman3, make_regression, make_friedman2, fetch_california_housing, load_diabetes, make_friedman1, make_sparse_uncorrelated, make_classification\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import log_loss, f1_score, auc, roc_auc_score, mean_squared_log_error\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(7)\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b76f9675-12af-4b17-b240-45d104b3039f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pandas import DataFrame, concat\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import NuSVR, SVC, SVR, LinearSVR\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LassoCV, RidgeCV, LarsCV, OrthogonalMatchingPursuitCV, LassoLarsCV, ElasticNet, ElasticNetCV, SGDRegressor, LassoLars, Lasso, Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from time import perf_counter\n",
    "# from lazypredict.Supervised import LazyClassifier\n",
    "from random import sample\n",
    "from copy import deepcopy\n",
    "\n",
    "class MSBoostClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"A Gradient Boosting Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\" Initialize MSBoost Object\n",
    "        \"\"\"\n",
    "\n",
    "    def _metrics(self, vt, vp, model, time=None):\n",
    "        \"\"\"get loss metrics of a model\n",
    "\n",
    "        Args:\n",
    "            vt (iterable): validation true values\n",
    "            vp (iterable): validation pred values\n",
    "            model (object): any model with fit and predict method\n",
    "            time (float, optional): execution time of the model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict['model', 'time', 'loss']\n",
    "        \"\"\"\n",
    "        if self.custom_loss_metrics:\n",
    "            return {'model': model, 'time': time, 'loss': self.custom_loss_metrics(vt, vp)}\n",
    "        return {\"model\": model, \"time\": time, \"loss\": mean_squared_error(vt, vp)}\n",
    "\n",
    "    def _create_model(self, X, y, model_name, time_it: bool = False):\n",
    "        \"\"\"fit a model instance\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterable)\n",
    "            model_name (object): any model object with fit and predict methods\n",
    "            time_it (bool, optional): measure execution time. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple(model, time=None)\n",
    "        \"\"\"\n",
    "        model = model_name()\n",
    "        if time_it:\n",
    "            begin = perf_counter()\n",
    "            model.fit(X, y)\n",
    "            end = perf_counter()\n",
    "            return (model, end - begin)\n",
    "        return (model.fit(X, y), None)\n",
    "\n",
    "    def _get_metrics(self, model_name):\n",
    "        \"\"\"a helper fuction, combines self._create_model and self._metrics\n",
    "\n",
    "        Args:\n",
    "            model_name (object): any model with fit and predict methods\n",
    "\n",
    "        Returns:\n",
    "            self._metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Xt, Xv, yt, yv = train_test_split(self._X, self._y)\n",
    "            results = self._create_model(Xt, yt, model_name, time_it=False)\n",
    "            model, time = results[0], results[1]\n",
    "            return self._metrics(yv,\n",
    "                                 model.predict(Xv), model, time)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_results(self, X, y) -> list:\n",
    "        \"\"\"Use multi-threading to return all results\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterable)\n",
    "\n",
    "        Returns:\n",
    "            list[dict['model', 'time', 'loss']]\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # self._X = self._minimax.fit_transform(self._robust.fit_transform(\n",
    "        #         KNNImputer(weights='distance').fit_transform(X)))\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        with ThreadPoolExecutor(max_workers=len(self._models)) as executor:\n",
    "            res = executor.map(self._get_metrics, self._models)\n",
    "            results = [i for i in res if i]\n",
    "        return results\n",
    "\n",
    "    def fit(\n",
    "        self, X, y,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_min_delta: float = 0.001,\n",
    "        early_stopping_patience: int = 10,\n",
    "        custom_models: list = None,\n",
    "        learning_rate: float = 0.1,\n",
    "        n_estimators: int = 100,\n",
    "        warm_start: bool = False,\n",
    "        complexity: bool = True,\n",
    "        light: bool = False,\n",
    "        custom_loss_metrics: object = False,\n",
    "        freeze_models: bool = False,\n",
    "        n_models: int = 5,\n",
    "        n_iter_models: int = 5,\n",
    "        n_warm: int = None,\n",
    "        n_random_models: int = 24,\n",
    "        return_vals: bool = True,\n",
    "    ):\n",
    "        \"\"\"fit MSBoost model\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterbale)\n",
    "            early_stopping (bool, optional): Defaults to False.\n",
    "            early_stopping_min_delta (float, optional): Defaults to 0.001.\n",
    "            early_stopping_patience (int, optional): Defaults to 10.\n",
    "            custom_models (tuple, optional): tuple of custom models with fit and predict methods. Defaults to None.\n",
    "            learning_rate (float, optional): Defaults to 0.05.\n",
    "            n_estimators (int, optional): Defaults to 100.\n",
    "            warm_start (bool, optional): Defaults to False.\n",
    "            complexity (bool, optional): trains more models but has greater time complexity. Defaults to False.\n",
    "            light (bool, optional): trains less models. Defaults to True.\n",
    "            custom_loss_metrics (object, optional): _description_. Defaults to False.\n",
    "            freeze_models (bool, optional): test only a selected models. Defaults to False.\n",
    "            n_models (int, optional): Applicable for freeze_models, number of models to train. Defaults to 5.\n",
    "            n_iter_models (int, optional): Applicable for freeze_models, number of iterations before finalizing the models. Defaults to 5.\n",
    "            n_warm (int, optional): Applicable for warm start, number of iterarions to store. Defaults to None.\n",
    "            n_random_models (int, optional): train on a random number of models. Defaults to 0.\n",
    "            return_vals (bool, optional): returns analytics. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tuple[final ensemble sequence, mean absolute error of each layer, residual value of each layer],\n",
    "            None\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = np.array(set(y))\n",
    "        self.y_max = max(y)\n",
    "        # self.n_classes_ = len(self.classes_)\n",
    "        self.len_X = X.shape[0]\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        if custom_models:\n",
    "            self._models = custom_models\n",
    "        self.custom_loss_metrics = custom_loss_metrics\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        if custom_models:\n",
    "\n",
    "            self._models_lst = custom_models\n",
    "        else:\n",
    "            if complexity:\n",
    "                self._models_lst = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, HistGradientBoostingRegressor,\n",
    "                                    ElasticNet, LassoLars, Lasso, GradientBoostingRegressor, ExtraTreesRegressor, SVC,\n",
    "                                    BaggingRegressor, NuSVR, XGBRegressor, SGDRegressor, KernelRidge, MLPRegressor, LGBMRegressor,\n",
    "                                    Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC)\n",
    "            elif light:\n",
    "                self._models_lst = (LGBMRegressor, ExtraTreesRegressor,\n",
    "                                     BaggingRegressor, RANSACRegressor, LassoLarsIC, BayesianRidge)\n",
    "            else:\n",
    "                self._models_lst = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, LGBMRegressor,\n",
    "                                    ElasticNet, LassoLars, Lasso, SGDRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "                                    Ridge, ARDRegression, RANSACRegressor, LassoLarsIC)\n",
    "            self._models = deepcopy(self._models_lst)\n",
    "        self.freeze_models = freeze_models\n",
    "        if self.freeze_models:\n",
    "            self.n_models = n_models\n",
    "            self.n_iter_models = n_iter_models\n",
    "        self._y_mean = 0.5\n",
    "        # base model: mean\n",
    "        # computer residuals: y - y hat\n",
    "        # for n_estimators: a) y = prev residuals && residuals * learning rate\n",
    "        # add early stopping\n",
    "        # restore best weights\n",
    "        # ada boost and adaptive scaling for learning rates\n",
    "        self._ensemble = []\n",
    "        preds = DataFrame(\n",
    "            data={'yt': y, 'p0': np.full((len(y)), y - self._y_mean)})\n",
    "        residuals = DataFrame(\n",
    "            data={'r0': y - self._y_mean})\n",
    "        errors = []\n",
    "        if not early_stopping:\n",
    "            if warm_start:\n",
    "                for i in range(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X, y)\n",
    "                    min_loss = min(results, key=lambda x: x.get(\n",
    "                        \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    if i % n_warm == 0:\n",
    "                        X[f\"r{i}\"] = residuals[f'r{i}'].copy()\n",
    "                    try:\n",
    "                        errors.append(mean_squared_error(\n",
    "                            preds['yt'], preds[f'p{i}']))\n",
    "                    except Exception:\n",
    "                        df = concat(\n",
    "                            [preds['yt'], preds[f'p{i - 1}']], axis=1).dropna()\n",
    "                        errors.append(mean_squared_error(\n",
    "                            df['yt'], df[f\"p{i - 1}\"]))\n",
    "                    self._ensemble.append(min_model)\n",
    "            else:\n",
    "                freeze_models_lst = []\n",
    "                for i in range(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X, y)\n",
    "                    if n_random_models > 0:\n",
    "                        self._models = tuple(\n",
    "                            sample(self._models_lst, n_random_models))\n",
    "                    elif self.freeze_models:\n",
    "                        if self.n_iter_models > -1:\n",
    "                            freeze_models_lst.append([i.get(\"model\") for i in sorted(results, key=lambda x: x.get(\n",
    "                                \"loss\", float('inf')))][:n_models])\n",
    "                            self.n_iter_models -= 1\n",
    "                        else:\n",
    "                            model_lst = sorted(dict(Counter(i for sub in freeze_models_lst for i in set(\n",
    "                                sub))).items(), key=lambda ele: ele[1], reverse=True)\n",
    "                            # return model_lst\n",
    "                            self._models = tuple(type(i[0]) for i in model_lst)[\n",
    "                                :n_models]\n",
    "                            # return self._models\n",
    "                    try:\n",
    "                        min_loss = min(results, key=lambda x: x.get(\n",
    "                            \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    errors.append(mean_squared_error(\n",
    "                        preds['yt'], preds[f'p{i}']))\n",
    "                    self._ensemble.append(min_model)\n",
    "                    if errors[i - 1] == 0:\n",
    "                        break\n",
    "        else:\n",
    "            return self\n",
    "        # min_error = min(errors)\n",
    "        # min_error_i = [i for i in range(\n",
    "        #     len(errors)) if errors[i] == min_error][0]\n",
    "        # self._ensemble, errors = self._ensemble[:\n",
    "        #                                         min_error_i], errors[:min_error_i]\n",
    "        # residuals = residuals[:len(errors)]\n",
    "        if return_vals:\n",
    "            self.residuls = residuals\n",
    "            self.errors = errors\n",
    "            self.ensemble = self._ensemble\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_test (iterable)\n",
    "\n",
    "        Returns:\n",
    "            numpy.array: predictions\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X_test = check_array(X_test)\n",
    "        # X_test = self._robust.transform(self._minimax.transform(deepcopy(X_test)))\n",
    "        return np.argmax(self.predict_proba(X_test), axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        #     # crude data\n",
    "        #     # dont quantize\n",
    "        #     # https://datascience.stackexchange.com/q/22762\n",
    "        #     # https://www.youtube.com/watch?v=ZsM2z0pTbnk\n",
    "        #     # https://www.youtube.com/watch?v=yJK4sYclhg8\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        preds = DataFrame(\n",
    "            data={'p0': np.full((len(X)), self._y_mean)})\n",
    "        for i in range(len(self._ensemble)):\n",
    "            preds[f\"p{i}\"] = self._ensemble[i].predict(X)\n",
    "        preds_ = MinMaxScaler().fit_transform(\n",
    "            preds.sum(axis=1).to_numpy().reshape(-1, 1))\n",
    "        preds_ = preds_.reshape(1, -1)[0]\n",
    "        proba = []\n",
    "        for i in preds_:\n",
    "            proba.append([1.0 - i, i])\n",
    "        return np.array(proba)\n",
    "\n",
    "\n",
    "def subsample(X, y, num_samples=1000, random_state=7):\n",
    "    \"\"\"\n",
    "    Subsample the arrays X and y.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array\n",
    "    - y: Target array\n",
    "    - num_samples: Number of samples to retain\n",
    "    - random_state: Seed for random number generator (default is None)\n",
    "\n",
    "    Returns:\n",
    "    - Subsampled X and y arrays\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.random.choice(len(y), num_samples, replace=False)\n",
    "\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def get_card_split(df, cols, n=11):\n",
    "    \"\"\"\n",
    "    Splits categorical columns into 2 lists based on cardinality (i.e # of unique values)\n",
    "    Parameters (Source: https://github.com/shankarpandala/lazypredict/blob/dev/lazypredict/Supervised.py#L114)\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which the cardinality of the columns is calculated.\n",
    "    cols : list-like\n",
    "        Categorical columns to list\n",
    "    n : int, optional (default=11)\n",
    "        The value of 'n' will be used to split columns.\n",
    "    Returns\n",
    "    -------\n",
    "    card_low : list-like\n",
    "        Columns with cardinality < n\n",
    "    card_high : list-like\n",
    "        Columns with cardinality >= n\n",
    "        \n",
    "    \"\"\"\n",
    "    cond = df[cols].nunique() > n\n",
    "    card_high = cols[cond]\n",
    "    card_low = cols[~cond]\n",
    "    return card_low, card_high\n",
    "\n",
    "def append_row(df, data):\n",
    "    \"\"\"\n",
    "    Append a row of data to a DataFrame.\n",
    "\n",
    "    Parameters :\n",
    "        - df (pandas.DataFrame): The DataFrame to which the row will be appended.\n",
    "        - data (list): The data representing a row to be appended. Should be a list where each element corresponds to a column in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = data\n",
    "    \n",
    "import os\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            \n",
    "def score_clf(model, X, y):\n",
    "    return log_loss(y, model.predict_proba(X))\n",
    "\n",
    "def pre_process_y(y):\n",
    "    return LabelEncoder().fit_transform(y)\n",
    "\n",
    "def cv_evaluate(model, X, y, scoring=\"neg_mean_squared_error\", n_jobs=1):\n",
    "\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    reg = model(n_estimators=100)\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=7)\n",
    "    with suppress_stdout():\n",
    "        cv_results = cross_val_score(reg, X, y, cv=kf, scoring=scoring, n_jobs=n_jobs)\n",
    "    mean_mse = np.mean(cv_results)\n",
    "    std_mse = np.std(cv_results)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    return f\"{mean_mse:.4f} ± {std_mse:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364eb0d2-0146-4346-9640-6da0ca8307b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, n_repeated=0, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6164aa-5ddd-4224-ac38-472679f1dc0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2499 ± 0.0495'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "cv_evaluate(LGBMClassifier, X, y, score_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd21ccb-e6bf-4e91-9abf-818659564a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2631 ± 0.0533'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "cv_evaluate(XGBClassifier, X, y, score_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b48d2cb-a1f4-492c-8b0f-ca405058561e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2153 ± 0.0398'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_evaluate(MSBoostClassifier, X, y, score_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed329f-50d8-446f-8a74-ccfa34d101a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_evaluate(MSBoostClassifier, X, y, score_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4a79a-c3c5-494f-b92f-4671f2359e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenML Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "730832b1-e1ab-44d2-b491-64e1f7839e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_id = [\n",
    "    151,\n",
    "     293,\n",
    "     722,\n",
    "     821,\n",
    "     1120,\n",
    "     1461,\n",
    "     4134,\n",
    "     41150,\n",
    "     42477,\n",
    "     42769,\n",
    "     1044,\n",
    "     4541,\n",
    "     41168,\n",
    "     45026,\n",
    "     45026,\n",
    "    44089,\n",
    "    45028,\n",
    "    151,\n",
    "    1044,\n",
    "    1596,\n",
    "    41147,\n",
    "    42192,\n",
    "    42477,\n",
    "    42803,\n",
    "    40536, \n",
    "    1489, \n",
    "    4134, \n",
    "    4532, \n",
    "    40981, \n",
    "    25, \n",
    "    41159, \n",
    "    1169,\n",
    "    41143,    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a52ed4-b4b5-4823-9754-0c72c6850941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from LazyPredict\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", KNNImputer(n_neighbors=15, weights=\"distance\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer_low = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoding\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer_high = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        # 'OrdianlEncoder' Raise a ValueError when encounters an unknown value. Check https://github.com/scikit-learn/scikit-learn/pull/13423\n",
    "        (\"encoding\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2065a99a-d56b-4160-93fe-945f1daa54d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_samples = 1000\n",
    "cols = [\"Dataset\", \"OpenML ID\", \"LGBM\", \"XGB\", \"MSBoost\"]\n",
    "benchmark = pd.DataFrame(columns=cols)\n",
    "lasso_threshold = 0.01 # remove irrelevant features \n",
    "invalid = []\n",
    "\n",
    "for i in range(len(data_id)):\n",
    "    try:\n",
    "        ID = data_id[i]\n",
    "        try:\n",
    "            data = fetch_openml(data_id=ID)\n",
    "        except:\n",
    "            continue\n",
    "        name = data[\"details\"][\"name\"]\n",
    "        df = data[\"frame\"]\n",
    "        target = data[\"target_names\"]\n",
    "        try:\n",
    "            if len(df) > num_samples:\n",
    "                df = df.sample(num_samples, random_state=7)\n",
    "        except:\n",
    "            continue\n",
    "        X, y = df.drop(target, axis=1), df[target]\n",
    "\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "        categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "        categorical_low, categorical_high = get_card_split(\n",
    "            X, categorical_features\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"numeric\", numeric_transformer, numeric_features),\n",
    "                (\"categorical_low\", categorical_transformer_low, categorical_low),\n",
    "                (\"categorical_high\", categorical_transformer_high, categorical_high),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        X = preprocessor.fit_transform(X)\n",
    "        y = pre_process_y(y[y.columns[0]])\n",
    "        lasso = LassoCV(cv=10, max_iter=10000)\n",
    "        lasso.fit(X, y)\n",
    "        coefficients = lasso.coef_\n",
    "        selected_features = np.where(np.abs(coefficients) > lasso_threshold)[0]\n",
    "        X = X[:, selected_features]\n",
    "        lgb = \"NaN\"\n",
    "        xgb = \"NaN\"\n",
    "        msboost = \"NaN\"\n",
    "        with suppress_stdout():\n",
    "            try:\n",
    "                lgb = cv_evaluate(LGBMClassifier, X, y, score_clf, n_jobs=-1)\n",
    "            except Exception as error:\n",
    "                invalid.append(i, error)\n",
    "            try:\n",
    "                xgb = cv_evaluate(XGBClassifier, X, y, score_clf, n_jobs=-1)\n",
    "            except Exception as error:\n",
    "                invalid.append([i, error])\n",
    "            try:\n",
    "                msboost = cv_evaluate(MSBoostClassifier, X, y, score_clf, n_jobs=-1)\n",
    "            except Exception as error:\n",
    "                invalid.append([i, error])\n",
    "            append_row(benchmark, [name, ID, lgb, xgb, msboost])\n",
    "            benchmark.to_csv(\"OpenML2.csv\", index=False)\n",
    "    except Exception as error:\n",
    "        invalid.append([i, error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c4d6f7-c7c2-4b3f-83c9-f6352279c8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>OpenML ID</th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGB</th>\n",
       "      <th>MSBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guillermo</td>\n",
       "      <td>41159</td>\n",
       "      <td>0.6988 ± 0.1070</td>\n",
       "      <td>0.7725 ± 0.1146</td>\n",
       "      <td>0.5644 ± 0.0461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airlines</td>\n",
       "      <td>1169</td>\n",
       "      <td>0.7761 ± 0.0613</td>\n",
       "      <td>0.8387 ± 0.0670</td>\n",
       "      <td>0.8986 ± 0.0709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jasmine</td>\n",
       "      <td>41143</td>\n",
       "      <td>0.5472 ± 0.0422</td>\n",
       "      <td>0.6198 ± 0.0401</td>\n",
       "      <td>0.5475 ± 0.1169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  OpenML ID             LGBM              XGB             MSBoost\n",
       "0  guillermo      41159  0.6988 ± 0.1070  0.7725 ± 0.1146  0.5644 ± 0.0461\n",
       "1   airlines       1169  0.7761 ± 0.0613  0.8387 ± 0.0670  0.8986 ± 0.0709\n",
       "2    jasmine      41143  0.5472 ± 0.0422  0.6198 ± 0.0401  0.5475 ± 0.1169"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f675436-4dd8-4a21-90b1-fa7c6d126999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark.to_csv(\"OpenML3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2904b3ec-1155-4a70-88da-fac5c93530b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = benchmark.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243abdd1-4913-4a0b-b9ac-d3db2fcde214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df[['LGBM', 'XGB', 'MSBoost']] = df[['LGBM', 'XGB', 'MSBoost']].apply(lambda x: x.str.split(' ± ').str[0]).astype(float)\n",
    "\n",
    "len(df[(df['MSBoost'] < df['LGBM']) & (df['MSBoost'] < df['XGB'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b10c314d-bd21-4eaa-9808-4f3bbcdc0850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['LGBM'] < df['MSBoost']) & (df['LGBM'] < df['XGB'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1cf445-3c08-4b23-a855-a9c46fa42727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>OpenML ID</th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGB</th>\n",
       "      <th>MSBoost</th>\n",
       "      <th>% Improvement in MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guillermo</td>\n",
       "      <td>41159</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>23.812899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airlines</td>\n",
       "      <td>1169</td>\n",
       "      <td>0.7761</td>\n",
       "      <td>0.8387</td>\n",
       "      <td>0.8986</td>\n",
       "      <td>-13.632317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jasmine</td>\n",
       "      <td>41143</td>\n",
       "      <td>0.5472</td>\n",
       "      <td>0.6198</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>-0.054795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  OpenML ID    LGBM     XGB    MSBoost  % Improvement in MSE\n",
       "0  guillermo      41159  0.6988  0.7725  0.5644             23.812899\n",
       "1   airlines       1169  0.7761  0.8387  0.8986            -13.632317\n",
       "2    jasmine      41143  0.5472  0.6198  0.5475             -0.054795"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"% Improvement in MSE\"] = ((df[\"LGBM\"] - df[\"MSBoost\"]) / df['MSBoost']) * 100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c4a83cc-ff03-4e3a-979c-a192b30cf238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.090290857989505"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df[\"% Improvement in MSE\"][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91424ab1-fd41-4784-9487-bddc75065767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = benchmark.copy()\n",
    "df[['LGBM', 'XGB', 'MSBoost']] = df[['LGBM', 'XGB', 'MSBoost']].apply(lambda x: x.str.split(' ± ').str[1]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4d23276-d7b4-469e-9fc6-a3ce548d6e88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['MSBoost'] < df['LGBM']) & (df['MSBoost'] < df['XGB'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfca22e0-1c14-4052-92c7-f5a2f96e1a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['LGBM'] < df['MSBoost']) & (df['LGBM'] < df['XGB'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b909c808-0ddd-49e3-ba50-4214e2d7a5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>OpenML ID</th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGB</th>\n",
       "      <th>MSBoost</th>\n",
       "      <th>% Improvement In Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guillermo</td>\n",
       "      <td>41159</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>132.104121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airlines</td>\n",
       "      <td>1169</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>-13.540197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jasmine</td>\n",
       "      <td>41143</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>-63.900770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  OpenML ID    LGBM     XGB    MSBoost  % Improvement In Variance\n",
       "0  guillermo      41159  0.1070  0.1146  0.0461                 132.104121\n",
       "1   airlines       1169  0.0613  0.0670  0.0709                 -13.540197\n",
       "2    jasmine      41143  0.0422  0.0401  0.1169                 -63.900770"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"% Improvement In Variance\"] = ((df[\"LGBM\"] - df[\"MSBoost\"]) / df['MSBoost']) * 100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99d2722b-1f0e-4a9c-b2af-b234254b085a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.28196200692062"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df[\"% Improvement In Variance\"][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b1ef7d6-2d53-4f91-be88-35d5d34dbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500952b-a377-42d2-ba6d-4245952bece2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
