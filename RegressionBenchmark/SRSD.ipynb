{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de6c7fc-b4bf-4e26-b9d4-13b16eea414c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/snape/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: datasets in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/snape/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ac3c3e-f6f4-4a56-817c-945aabfde9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pandas import DataFrame, concat\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.svm import NuSVR, SVC, SVR, LinearSVR\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LassoCV, RidgeCV, LarsCV, OrthogonalMatchingPursuitCV, LassoLarsCV, ElasticNet, ElasticNetCV, SGDRegressor, LassoLars, Lasso, Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from time import perf_counter\n",
    "from random import sample\n",
    "from copy import deepcopy\n",
    "\n",
    "class MSBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"A Gradient Boosting Regressor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize VGBRegressor Object\n",
    "        \"\"\"\n",
    "\n",
    "    def _metrics(self, vt, vp, model, time=None):\n",
    "        \"\"\"get loss metrics of a model\n",
    "\n",
    "        Args:\n",
    "            vt (iterable): validation true values\n",
    "            vp (iterable): validation pred values\n",
    "            model (object): any model with fit and predict method\n",
    "            time (float, optional): execution time of the model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict['model', 'time', 'loss']\n",
    "        \"\"\"\n",
    "        if self.custom_loss_metrics:\n",
    "            return {'model': model, 'time': time, 'loss': self.custom_loss_metrics(vt, vp)}\n",
    "        return {\"model\": model, \"time\": time, \"loss\": mean_squared_error(vt, vp)}\n",
    "\n",
    "    def _create_model(self, X, y, model_name, time_it: bool = False):\n",
    "        \"\"\"fit a model instance\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterable)\n",
    "            model_name (object): any model object with fit and predict methods\n",
    "            time_it (bool, optional): measure execution time. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple(model, time=None)\n",
    "        \"\"\"\n",
    "        model = model_name()\n",
    "        if time_it:\n",
    "            begin = perf_counter()\n",
    "            model.fit(X, y)\n",
    "            end = perf_counter()\n",
    "            return (model, end - begin)\n",
    "        return (model.fit(X, y), None)\n",
    "\n",
    "    def _get_metrics(self, model_name):\n",
    "        \"\"\"a helper fuction, combines self._create_model and self._metrics\n",
    "\n",
    "        Args:\n",
    "            model_name (object): any model with fit and predict methods\n",
    "\n",
    "        Returns:\n",
    "            self._metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Xt, Xv, yt, yv = train_test_split(self._X, self._y)\n",
    "            results = self._create_model(Xt, yt, model_name, time_it=False)\n",
    "            model, time = results[0], results[1]\n",
    "            return self._metrics(yv,\n",
    "                                 model.predict(Xv), model, time)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_results(self, X, y) -> list:\n",
    "        \"\"\"Use multi-threading to return all results\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterable)\n",
    "\n",
    "        Returns:\n",
    "            list[dict['model', 'time', 'loss']]\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # self._X = self._minimax.fit_transform(self._robust.fit_transform(\n",
    "        #         KNNImputer(weights='distance').fit_transform(X)))\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        with ThreadPoolExecutor(max_workers=len(self._models)) as executor:\n",
    "            res = executor.map(self._get_metrics, self._models)\n",
    "            results = [i for i in res if i]\n",
    "        return results\n",
    "\n",
    "    def fit(\n",
    "        self, X, y,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_min_delta: float = 0.001,\n",
    "        early_stopping_patience: int = 10,\n",
    "        custom_models: list = None,\n",
    "        learning_rate: float = 0.05,\n",
    "        n_estimators: int = 100,\n",
    "        warm_start: bool = False,\n",
    "        complexity: bool = True,\n",
    "        light: bool = False,\n",
    "        custom_loss_metrics: object = False,\n",
    "        freeze_models: bool = False,\n",
    "        n_models: int = 5,\n",
    "        n_iter_models: int = 5,\n",
    "        n_warm: int = None,\n",
    "        n_random_models: int = 12,\n",
    "        return_vals: bool = True,\n",
    "        # stacking_model=ExtraTreesRegressor,\n",
    "    ):\n",
    "        \"\"\"fit VGBoost model\n",
    "\n",
    "        Args:\n",
    "            X (iterable)\n",
    "            y (iterbale)\n",
    "            early_stopping (bool, optional): Defaults to False.\n",
    "            early_stopping_min_delta (float, optional): Defaults to 0.001.\n",
    "            early_stopping_patience (int, optional): Defaults to 10.\n",
    "            custom_models (tuple, optional): tuple of custom models with fit and predict methods. Defaults to None.\n",
    "            learning_rate (float, optional): Defaults to 0.05.\n",
    "            n_estimators (int, optional): Defaults to 100.\n",
    "            warm_start (bool, optional): Defaults to False.\n",
    "            complexity (bool, optional): trains more models but has greater time complexity. Defaults to False.\n",
    "            light (bool, optional): trains less models. Defaults to True.\n",
    "            custom_loss_metrics (object, optional): _description_. Defaults to False.\n",
    "            freeze_models (bool, optional): test only a selected models. Defaults to False.\n",
    "            n_models (int, optional): Applicable for freeze_models, number of models to train. Defaults to 5.\n",
    "            n_iter_models (int, optional): Applicable for freeze_models, number of iterations before finalizing the models. Defaults to 5.\n",
    "            n_warm (int, optional): Applicable for warm start, number of iterarions to store. Defaults to None.\n",
    "            n_random_models (int, optional): train on a random number of models. Defaults to 0.\n",
    "            return_vals (bool, optional): returns analytics. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tuple[final ensemble sequence, mean absolute error of each layer, residual value of each layer],\n",
    "            None\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = np.array(set(y))\n",
    "        # self.stacking_model = stacking_model\n",
    "        self.y_max = max(y)\n",
    "        # self.n_classes_ = len(self.classes_)\n",
    "        self.len_X = X.shape[0]\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        if custom_models:\n",
    "            self._models = custom_models\n",
    "        self.custom_loss_metrics = custom_loss_metrics\n",
    "        self.learning_rate = learning_rate\n",
    "        # self.final_estimator = final_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        if custom_models:\n",
    "\n",
    "            self._models_lst = custom_models\n",
    "        else:\n",
    "            if complexity:\n",
    "                self._models_lst = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, HistGradientBoostingRegressor, LGBMRegressor, GradientBoostingRegressor, XGBRegressor,\n",
    "                                    ElasticNetCV, LassoLarsCV, LassoCV, ExtraTreesRegressor, SVC, \n",
    "                                    BaggingRegressor, NuSVR, SGDRegressor, KernelRidge, MLPRegressor,\n",
    "                                    RidgeCV, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC)\n",
    "            elif light:\n",
    "                self._models_lst = (LGBMRegressor, ExtraTreesRegressor,\n",
    "                                    BaggingRegressor, RANSACRegressor, LassoLarsIC, BayesianRidge)\n",
    "            else:\n",
    "                self._models_lst = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, LGBMRegressor,\n",
    "                                    ElasticNet, LassoLars, Lasso, SGDRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "                                    Ridge, ARDRegression, RANSACRegressor, LassoLarsIC)\n",
    "            self._models = deepcopy(self._models_lst)\n",
    "        self.freeze_models = freeze_models\n",
    "        if self.freeze_models:\n",
    "            self.n_models = n_models\n",
    "            self.n_iter_models = n_iter_models\n",
    "        self._y_mean = 0\n",
    "        # base model: mean\n",
    "        # computer residuals: y - y hat\n",
    "        # for n_estimators: a) y = prev residuals && residuals * learning rate\n",
    "        # add early stopping\n",
    "        # restore best weights\n",
    "        # ada boost and adaptive scaling for learning rates\n",
    "        self._ensemble = []\n",
    "        preds = DataFrame(\n",
    "            data={'yt': y, 'p0': np.full((len(y)), self._y_mean)})\n",
    "        residuals = DataFrame(\n",
    "            data={'r0': y - self._y_mean})\n",
    "        errors = []\n",
    "        if not early_stopping:\n",
    "            if warm_start:\n",
    "                # for i in range(1, self.n_estimators + 1):\n",
    "                #     try:\n",
    "                #         y = residuals[f'r{i - 1}']\n",
    "                #     except KeyError:\n",
    "                #         return residuals\n",
    "                #     results = self._get_results(X, y)\n",
    "                #     min_loss = min(results, key=lambda x: x.get(\n",
    "                #         \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                #     min_model = [i['model']\n",
    "                #                  for i in results if min_loss >= i['loss']][0]\n",
    "                #     preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                #         X) * self.learning_rate\n",
    "                #     residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                #     if i % n_warm == 0:\n",
    "                #         X[f\"r{i}\"] = residuals[f'r{i}'].copy()\n",
    "                #     try:\n",
    "                #         errors.append(mean_squared_error(\n",
    "                #             preds['yt'], preds[f'p{i}']))\n",
    "                #     except Exception:\n",
    "                #         df = concat(\n",
    "                #             [preds['yt'], preds[f'p{i - 1}']], axis=1).dropna()\n",
    "                #         errors.append(mean_squared_error(\n",
    "                #             df['yt'], df[f\"p{i - 1}\"]))\n",
    "                #     self._ensemble.append(min_model)\n",
    "                pass\n",
    "            else:\n",
    "                freeze_models_lst = []\n",
    "                for i in range(1, self.n_estimators + 1):\n",
    "                    try:\n",
    "                        y = residuals[f'r{i - 1}']\n",
    "                    except KeyError:\n",
    "                        return residuals, i, self._ensemble\n",
    "                    results = self._get_results(X, y)\n",
    "                    if n_random_models > 0:\n",
    "                        self._models = tuple(\n",
    "                            sample(self._models_lst, n_random_models))\n",
    "                    elif self.freeze_models:\n",
    "                        if self.n_iter_models > -1:\n",
    "                            freeze_models_lst.append([i.get(\"model\") for i in sorted(results, key=lambda x: x.get(\n",
    "                                \"loss\", float('inf')))][:n_models])\n",
    "                            self.n_iter_models -= 1\n",
    "                        else:\n",
    "                            model_lst = sorted(dict(Counter(i for sub in freeze_models_lst for i in set(\n",
    "                                sub))).items(), key=lambda ele: ele[1], reverse=True)\n",
    "                            # return model_lst\n",
    "                            self._models = tuple(type(i[0]) for i in model_lst)[\n",
    "                                :n_models]\n",
    "                            # return self._models\n",
    "                    try:\n",
    "                        min_loss = min(results, key=lambda x: x.get(\n",
    "                            \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    errors.append(mean_squared_error(\n",
    "                        preds['yt'], preds[f'p{i}']))\n",
    "                    self._ensemble.append(min_model)\n",
    "                    if errors[i - 1] == 0:\n",
    "                        break\n",
    "        else:\n",
    "            return self\n",
    "        min_error = min(errors)\n",
    "        min_error_i = [i for i in range(\n",
    "            len(errors)) if errors[i] == min_error][0]\n",
    "        self._ensemble, errors = self._ensemble[:\n",
    "                                                min_error_i], errors[:min_error_i]\n",
    "        residuals = residuals[:len(errors)]\n",
    "        # preds = preds[preds.columns[:min_error_i + 2]]\n",
    "        if return_vals:\n",
    "            self.residuls = residuals\n",
    "            self.errors = errors\n",
    "            self.ensemble = self._ensemble\n",
    "        # X, y = preds.drop(\"yt\", axis=1), preds[\"yt\"]\n",
    "        # self.preds = X\n",
    "        # self.final_estimator.fit(X, y)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_test (iterable)\n",
    "\n",
    "        Returns:\n",
    "            numpy.array: predictions\n",
    "        \"\"\"\n",
    "        # check_is_fitted(self)\n",
    "        X_test = check_array(X_test)\n",
    "        # X_test = self._robust.transform(self._minimax.transform(deepcopy(X_test)))\n",
    "        preds = DataFrame(\n",
    "            data={'p0': np.full((len(X_test)), self._y_mean)})\n",
    "        for i in range(len(self._ensemble)):\n",
    "            preds[f\"p{i + 1}\"] = self._ensemble[i].predict(X_test)\n",
    "        preds_ = preds.sum(axis=1)\n",
    "        return preds_\n",
    "\n",
    "\n",
    "def subsample(X, y, num_samples=1000, random_state=7):\n",
    "    \"\"\"\n",
    "    Subsample the arrays X and y.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array\n",
    "    - y: Target array\n",
    "    - num_samples: Number of samples to retain\n",
    "    - random_state: Seed for random number generator (default is None)\n",
    "\n",
    "    Returns:\n",
    "    - Subsampled X and y arrays\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.random.choice(len(y), num_samples, replace=False)\n",
    "\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def reg_cv_mse(model, X, y):\n",
    "    reg = model()\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=7)\n",
    "    cv_results = cross_val_score(reg, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    cv_results = -cv_results\n",
    "    mean_mse = np.mean(cv_results)\n",
    "    std_mse = np.std(cv_results)\n",
    "    return f\"{mean_mse:.4f} ± {std_mse:.4f}\"\n",
    "\n",
    "def get_card_split(df, cols, n=11):\n",
    "    \"\"\"\n",
    "    Splits categorical columns into 2 lists based on cardinality (i.e # of unique values)\n",
    "    Parameters (Source: https://github.com/shankarpandala/lazypredict/blob/dev/lazypredict/Supervised.py#L114)\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which the cardinality of the columns is calculated.\n",
    "    cols : list-like\n",
    "        Categorical columns to list\n",
    "    n : int, optional (default=11)\n",
    "        The value of 'n' will be used to split columns.\n",
    "    Returns\n",
    "    -------\n",
    "    card_low : list-like\n",
    "        Columns with cardinality < n\n",
    "    card_high : list-like\n",
    "        Columns with cardinality >= n\n",
    "        \n",
    "    \"\"\"\n",
    "    cond = df[cols].nunique() > n\n",
    "    card_high = cols[cond]\n",
    "    card_low = cols[~cond]\n",
    "    return card_low, card_high\n",
    "\n",
    "def append_row(df, data):\n",
    "    \"\"\"\n",
    "    Append a row of data to a DataFrame.\n",
    "\n",
    "    Parameters :\n",
    "        - df (pandas.DataFrame): The DataFrame to which the row will be appended.\n",
    "        - data (list): The data representing a row to be appended. Should be a list where each element corresponds to a column in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81095fe4-cc16-4d48-a849-6fbe5e978a4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d467278a8f6d4bcdaa51f2a32bcbead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508555976224423b55b62cfd29d7392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83e043fe54844ebbb42d4158c3f8710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yoshitomo-matsubara/srsd-feynman_hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df316b1f-8189-491c-9dc9-9d27fe8dd35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_string_to_numbers(data_strings):\n",
    "    # Split the string by whitespace and convert to floats\n",
    "    data_floats = [[float(num) for num in sample.split()] for sample in data_strings]\n",
    "\n",
    "    # Find the maximum number of elements in a row\n",
    "    max_length = max(len(row) for row in data_floats)\n",
    "\n",
    "    # Pad rows with zeros to ensure they all have the same length\n",
    "    padded_data = [row + [0.0] * (max_length - len(row)) for row in data_floats]\n",
    "\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def get_X_y(data_strings):\n",
    "    y = np.array([float(i.split()[-1]) for i in data_strings])\n",
    "    X_str = [\" \".join(i.split()[:-1]) for i in data_strings]\n",
    "    X = convert_string_to_numbers(X_str)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3cfed0-e7d1-4f96-bf89-169d265aa3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(dataset['train']['text'])\n",
    "X_test, y_test = get_X_y(dataset['test']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a1628d-c268-43e5-81ff-4790519ecbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce6b2db9-376f-4724-afda-046dcdc35135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_sub, y_train_sub = subsample(X_train, y_train, num_samples=400_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33c31d7-bee2-4b60-a292-044335905610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 8997674454850113652432969983328256.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb = LGBMRegressor()\n",
    "lgb.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375a235e-3944-46dd-b502-446c92396f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5819108802992412e+70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c26a50c-f6b0-4d56-85a5-13371b6e0c51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a0277c-5567-4841-abcd-db8f95040a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.984438123427946e+70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31008795-14dc-4c0d-8b75-18ede754d7df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snape/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/utils/_array_api.py:245: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 300000, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 9172317592102724674222073344688128.000000\n"
     ]
    }
   ],
   "source": [
    "msboost = MSBoostRegressor()\n",
    "msboost.fit(X_train_sub, y_train_sub, complexity=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc428e0d-4e2e-4fe4-98e9-11ea0c7bb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, msboost.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804250cb-0e1c-42f4-b874-1cef7328ab31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
